{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
    "# 2016 Artem Oboturov(oboturov@gmail.com)                             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    # @kArm: # of arms\n",
    "    # @epsilon: probability for exploration in epsilon-greedy algorithm\n",
    "    # @initial: initial estimation for each action\n",
    "    # @stepSize: constant step size for updating estimations\n",
    "    # @sampleAverages: if True, use sample averages to update estimations instead of constant step size\n",
    "    # @UCB: if not None, use UCB algorithm to select action\n",
    "    # @gradient: if True, use gradient based bandit algorithm\n",
    "    # @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm\n",
    "    def __init__(self, kArm=10, epsilon=0., initial=0., stepSize=0.1, sampleAverages=False, UCBParam=None,\n",
    "                 gradient=False, gradientBaseline=False, trueReward=0.):\n",
    "        self.k = kArm\n",
    "        self.stepSize = stepSize\n",
    "        self.sampleAverages = sampleAverages\n",
    "        self.indices = np.arange(self.k)\n",
    "        self.time = 0\n",
    "        self.UCBParam = UCBParam\n",
    "        self.gradient = gradient\n",
    "        self.gradientBaseline = gradientBaseline\n",
    "        self.averageReward = 0\n",
    "        self.trueReward = trueReward\n",
    "\n",
    "        # real reward for each action\n",
    "        self.qTrue = []\n",
    "\n",
    "        # estimation for each action\n",
    "        self.qEst = np.zeros(self.k)\n",
    "\n",
    "        # # of chosen times for each action\n",
    "        self.actionCount = []\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # initialize real rewards with N(0,1) distribution and estimations with desired initial value\n",
    "        for i in range(0, self.k):\n",
    "            self.qTrue.append(np.random.randn() + trueReward)\n",
    "            self.qEst[i] = initial\n",
    "            self.actionCount.append(0)\n",
    "\n",
    "        self.bestAction = np.argmax(self.qTrue)\n",
    "\n",
    "    # get an action for this bandit, explore or exploit?\n",
    "    def getAction(self):\n",
    "        # explore\n",
    "        if self.epsilon > 0:\n",
    "            if np.random.binomial(1, self.epsilon) == 1:\n",
    "                return np.random.choice(self.indices)\n",
    "\n",
    "        # exploit\n",
    "        if self.UCBParam is not None:\n",
    "            UCBEst = self.qEst + \\\n",
    "                     self.UCBParam * np.sqrt(np.log(self.time + 1) / (np.asarray(self.actionCount) + 1))\n",
    "            return np.argmax(UCBEst)\n",
    "        if self.gradient:\n",
    "            expEst = np.exp(self.qEst)\n",
    "            self.actionProb = expEst / np.sum(expEst)\n",
    "            return np.random.choice(self.indices, p=self.actionProb)\n",
    "        return np.argmax(self.qEst)\n",
    "\n",
    "    # take an action, update estimation for this action\n",
    "    def takeAction(self, action):\n",
    "        # generate the reward under N(real reward, 1)\n",
    "        reward = np.random.randn() + self.qTrue[action]\n",
    "        self.time += 1\n",
    "        self.averageReward = (self.time - 1.0) / self.time * self.averageReward + reward / self.time\n",
    "        self.actionCount[action] += 1\n",
    "\n",
    "        if self.sampleAverages:\n",
    "            # update estimation using sample averages\n",
    "            self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action])\n",
    "        elif self.gradient:\n",
    "            oneHot = np.zeros(self.k)\n",
    "            oneHot[action] = 1\n",
    "            if self.gradientBaseline:\n",
    "                baseline = self.averageReward\n",
    "            else:\n",
    "                baseline = 0\n",
    "            self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)\n",
    "        else:\n",
    "            # update estimation with constant step size\n",
    "            self.qEst[action] += self.stepSize * (reward - self.qEst[action])\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "figureIndex = 0\n",
    "\n",
    "# for figure 2.1\n",
    "def figure2_1():\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    sns.violinplot(data=np.random.randn(200,10) + np.random.randn(10))\n",
    "    plt.xlabel(\"Action\")\n",
    "    plt.ylabel(\"Reward distribution\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def banditSimulation(nBandits, time, bandits):\n",
    "    bestActionCounts = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))]\n",
    "    averageRewards = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))]\n",
    "    for banditInd, bandit in enumerate(bandits):\n",
    "        for i in range(0, nBandits):\n",
    "            for t in range(0, time):\n",
    "                action = bandit[i].getAction()\n",
    "                reward = bandit[i].takeAction(action)\n",
    "                averageRewards[banditInd][t] += reward\n",
    "                if action == bandit[i].bestAction:\n",
    "                    bestActionCounts[banditInd][t] += 1\n",
    "        bestActionCounts[banditInd] /= nBandits\n",
    "        averageRewards[banditInd] /= nBandits\n",
    "    return bestActionCounts, averageRewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for figure 2.2\n",
    "def epsilonGreedy(nBandits, time):\n",
    "    epsilons = [0, 0.1, 0.01]\n",
    "    bandits = []\n",
    "    for epsInd, eps in enumerate(epsilons):\n",
    "        bandits.append([Bandit(epsilon=eps, sampleAverages=True) for _ in range(0, nBandits)])\n",
    "    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for eps, counts in zip(epsilons, bestActionCounts):\n",
    "        plt.plot(counts, label='epsilon = '+str(eps))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for eps, rewards in zip(epsilons, averageRewards):\n",
    "        plt.plot(rewards, label='epsilon = '+str(eps))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('average reward')\n",
    "    plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for figure 2.3\n",
    "def optimisticInitialValues(nBandits, time):\n",
    "    bandits = [[], []]\n",
    "    bandits[0] = [Bandit(epsilon=0, initial=5, stepSize=0.1) for _ in range(0, nBandits)]\n",
    "    bandits[1] = [Bandit(epsilon=0.1, initial=0, stepSize=0.1) for _ in range(0, nBandits)]\n",
    "    bestActionCounts, _ = banditSimulation(nBandits, time, bandits)\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    plt.plot(bestActionCounts[0], label='epsilon = 0, q = 5')\n",
    "    plt.plot(bestActionCounts[1], label='epsilon = 0.1, q = 0')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for figure 2.4\n",
    "def ucb(nBandits, time):\n",
    "    bandits = [[], []]\n",
    "    bandits[0] = [Bandit(epsilon=0, stepSize=0.1, UCBParam=2) for _ in range(0, nBandits)]\n",
    "    bandits[1] = [Bandit(epsilon=0.1, stepSize=0.1) for _ in range(0, nBandits)]\n",
    "    _, averageRewards = banditSimulation(nBandits, time, bandits)\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    plt.plot(averageRewards[0], label='UCB c = 2')\n",
    "    plt.plot(averageRewards[1], label='epsilon greedy epsilon = 0.1')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for figure 2.5\n",
    "def gradientBandit(nBandits, time):\n",
    "    bandits =[[], [], [], []]\n",
    "    bandits[0] = [Bandit(gradient=True, stepSize=0.1, gradientBaseline=True, trueReward=4) for _ in range(0, nBandits)]\n",
    "    bandits[1] = [Bandit(gradient=True, stepSize=0.1, gradientBaseline=False, trueReward=4) for _ in range(0, nBandits)]\n",
    "    bandits[2] = [Bandit(gradient=True, stepSize=0.4, gradientBaseline=True, trueReward=4) for _ in range(0, nBandits)]\n",
    "    bandits[3] = [Bandit(gradient=True, stepSize=0.4, gradientBaseline=False, trueReward=4) for _ in range(0, nBandits)]\n",
    "    bestActionCounts, _ = banditSimulation(nBandits, time, bandits)\n",
    "    labels = ['alpha = 0.1, with baseline',\n",
    "              'alpha = 0.1, without baseline',\n",
    "              'alpha = 0.4, with baseline',\n",
    "              'alpha = 0.4, without baseline']\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for i in range(0, len(bandits)):\n",
    "        plt.plot(bestActionCounts[i], label=labels[i])\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% Optimal action')\n",
    "    plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2.6\n",
    "def figure2_6(nBandits, time):\n",
    "    labels = ['epsilon-greedy', 'gradient bandit',\n",
    "              'UCB', 'optimistic initialization']\n",
    "    generators = [lambda epsilon: Bandit(epsilon=epsilon, sampleAverages=True),\n",
    "                  lambda alpha: Bandit(gradient=True, stepSize=alpha, gradientBaseline=True),\n",
    "                  lambda coef: Bandit(epsilon=0, stepSize=0.1, UCBParam=coef),\n",
    "                  lambda initial: Bandit(epsilon=0, initial=initial, stepSize=0.1)]\n",
    "    parameters = [np.arange(-7, -1, dtype=np.float),\n",
    "                  np.arange(-5, 2, dtype=np.float),\n",
    "                  np.arange(-4, 3, dtype=np.float),\n",
    "                  np.arange(-2, 3, dtype=np.float)]\n",
    "\n",
    "    bandits = [[generator(pow(2, param)) for _ in range(0, nBandits)] for generator, parameter in zip(generators, parameters) for param in parameter]\n",
    "    _, averageRewards = banditSimulation(nBandits, time, bandits)\n",
    "    rewards = np.sum(averageRewards, axis=1)/time\n",
    "\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    i = 0\n",
    "    for label, parameter in zip(labels, parameters):\n",
    "        l = len(parameter)\n",
    "        plt.plot(parameter, rewards[i:i+l], label=label)\n",
    "        i += l\n",
    "    plt.xlabel('Parameter(2^x)')\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "figure2_1()\n",
    "epsilonGreedy(2000, 1000)\n",
    "optimisticInitialValues(2000, 1000)\n",
    "ucb(2000, 1000)\n",
    "gradientBandit(2000, 1000)\n",
    "\n",
    "# This will take somehow a long time\n",
    "figure2_6(2000, 1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
